


Phase 1 performs a coarse linear ranking to ensure fast and high recall 
article selection. The top 200 articles are passed
to Phase 2, which performs more delicate ranking. The number 200 was selected 
according to Phase 2 complexity and 
system latency constraints. 

Phase 2 ranking model integrates various type of features, including recency 
(approximated by document age), article popularity (gmp), and content based 
features. The latter includes total 
numbers of entities and categories, sum of aboutness scores and other 
arithmetic transformations of aboutness scores. Similar features are generated 
based on user profile such as number of user inferred entities and categories, 
sum of sparse polarity scores, and related arithmetic transformations of 
sparse polarity scores.

Correlations between the user profile and the document profile give rise to a 
few more features. Other than a single dot product used in Phase 1 that 
combines entities and categories, Phase 2 separates features from entity and 
from category. For example, the number of matched entities between the user 
and the document, the number of matched categories, dot product of sparse 
polarity and aboutness scores between entities, dot product between 
categories, and the total dot product between all entities and categories.

A very special feature that is unique for our system is the identity of the 
stream, or stream-id.  This is a categorical feature. The feature defines 
which stream the article belongs to. In the hiearchical 
structure~\ref{hierarchy}, each node is assigned an identification number. All 
articles in this stream have the same ID. The feature is assigned values 
according to its source. If it is from NFL, then stream-id=``nfl". We will 
show in the experiments this feature is very useful.  


The training is based on clicks. To collect training data, we set up a data 
collection bucket to get online users' response to recommended articles. The 
ranking of the 200 articles are completely randomized to eliminate position 
bias ~\cite{Li:2010:CAP:1772690.1772758}. The 
articles are presented to a user in the format of a scrollable stream. Only 
title and a short summary are shown. Among the articles not clicked, only the 
ones above the last clicked article (skipped ones) are used for training, 
since users most likely have viewed but decided not to click on them.

Our training data generation is formalized as follows. If user $i$ clicks some 
articles in the stream, we generate training examples, 
${(x_{0}^{i},y_{0}^{i},w_{0}^{i}),(x_{1}^{i},y_{1}^{i},w_{1}^{i}),\cdots,(x_{j}^{i},y_{j}^{i},w_{j}^{i}),\cdots,(x_{N}^{i},y_{N}^{i},w_{N}^{i})}$.
 Each example consists of triplet produced by user $i$ on article $j$. 
$x_{j}^{i}$ corresponds to feature vector of the $j$th article and the user 
$i$ . $y_{j}^{i}$ is a $\{0,1\}$ binary-valued label. If $j$ is a clicked 
article, 
then $y_{j}^{i}=1$. It is 0 otherwise.  $w_{j}^{i}$ is the weight of the 
training example, whose value is equal to one plus the logarithm of the 
dwell-time of the click event. The latter is defined to be the time spent 
reading the full article $j$, that is, the duration from when user $i$ clicks 
the article to when he leaves the article.  Only clicked articles have 
dwell-time. $w_{j}^{i}=1$ for skipped articles.  $N$ is the index of the last 
clicked article. The total article number is 200, but in reality $N \le 200$ 
due to discarding non-viewed articles from training. 


We employ Gradient Boosted Decision Tree (GBDT) 
algorithm~\cite{Friedman:2002,Ye:2009} as the learning method of choice. This 
algorithm has higher precision, not sensitive to over-training, and supports 
category features, for instance, stream-id. GBDT is an
additive regression algorithm consisting of an ensemble of trees,
fitted to current residuals, gradients of the loss function, in a
forward step-wise manner.



