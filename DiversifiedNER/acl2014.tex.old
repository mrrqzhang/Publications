\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{epsfig}

\begin{document}



%title{Modeling for Query Linguistic Analysis System (QLAS)}
%\author{Ruiqiang Zhang Yi Chang\\
%Yahoo! Inc \\
%701 First Avenue, Sunnyvale, CA94089 }
%\date{}




\title{Diversified Named Entity Recognition for Web Search Query}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\maketitle


\begin{abstract}
Conditional Random Field (CRF) is a well-known  approach for many natural language processing tasks including named entities recognition (NER) because of its high accuracy. However, if this approach is used into NER for web query, our research found it was not appropriate to use it as a stand-alone method. Due to query intent ambigurity, CRF-based NER has lower precision for queries with multiple intents, which is very common for queries. 
 This paper proposes a method based on gradient boosted decision tree (GBDT) to re-score method the results generated by CRF NER.  We found this approach significantly reduced error rate of multiple intent queries.
\end{abstract}





\section{Introduction}

Query is ambiguous. Web search users usually send short and ambiguous queries to get quick search results. "Apple" can mean different things by user. It could be a fruit (Food), or apple product (Brand) or apple corporate (Business). Discovering query intent is an important factor for improving Web search user experience. All the ambiguous query intents should be considered by search engines.  A good search engine can give diversified search results. He can understand and translate  queries into multiple categories. Top search engines such as Google, Bing and Yahoo can present users with results from different categories. Some typical categories include Product, Shopping, Travel, Local, etc.  The search results from specific categories are usually positioned on the top before Web search results on search result page. Therefore, given their important layout, understanding query intent is critical in these tasks. They are called Direct Display (DD) by some search engines.   DD Search retrieves content from storages of a specific category, which is from web search that indexes whole web contents.

%% If a query has shopping intent, Category Search shows search results from shopping listings.

NER is one of the big techniques in query intent understanding. The most widely used NER method is CRF which has  the state-of-the-art performance in text-based NER, as reported~\cite{McCallum:2003:ERN:1119176.1119206}. The open source NER toolkit developed by Stanford University uses CRF as the main algorithm~\footnote{http://nlp.stanford.edu/software/CRF-NER.shtml}.  Query NER is quite different from text in that it is short, ambiguous and non-grammatical. The task difficulty of query NER is higher than that of text NER if they are evaluated in terms of perplexity value. Perplexity of trigram language model in our work on Web search queries is 385, while the value is 141 for normal English text (evaluated by WSJ treebank~\cite{Mikolov:2012}).  There is less rich contextual information available in queies than texts makes query NER have lower accuracy. 
Aside from low accuracy, we found another serious issue was query intent ambigurity. If a query has multiple intent, all entity types have to be generated. Otherwise, search results may be biased to one intent and lose other intents.  There are many cases that all query intents are need to generate. This situation has not been considered in the existing NER work, especially for text based NER. Traditional NER only considers generating the best entity type. This is weakness of all existing methods including CRF. Even if CRF generates multiple (top 3) results, those results can be very similar. It is because CRF output scoring is sum of all query word's score. The whole query score is very likely to be dominated by a single word score. That is, the global score is controlled by one strong local maxima.   Due to this, multiple output can have very similar results. Diversity is an inherent problem for CRF. We will further explain this in section~\ref{sec:}.
 An example is shown in Table~\ref{table:diversity}. For the query "napa auto repair", "napa"'s meaning is ambiguous. It could be a city name or a business name.  Table~\ref{table:diversity} lists five interpretations from CRF. All of the top three interpretations tag "napa" as PLACE because it is a dominated entity. The last two labels "napa" as ORGANIZATION. But the last two can probably be missed if only top three are used. 

In light of CRF's diversity problem on query NER, this work adopts a decision tree based method on the ground of  CRF based results. In this framework, CRF's role plays as one of query interpretation generation methods, together with other two methods: dictionary based and rule-based. All three methods each generates a few candidate interpretations and then the process is followed by a decision tree based re-ranking. We used gradient boosted decision tree (GBDT)\cite{Friedman:2002,Ye:2009}  model. We found this model could reduce diversity problem by two reasons: one is global features and using editorial subjective judgement as training target. We give details in section~\cite{section:}.

\begin{table*}
\begin{center}
\begin{tabular}{|c|l|c|} \hline
 & interpretation & crf score \\ \hline
1 & [napa]\_PLACE  [auto repair]\_BUSINESS & \\ \hline
2 & [napa]\_PALCE  [auto]\_BUSINESS    [repair]\_token & \\ \hline 
3 & [napa]\_PLACE  [auto]\_token  [repair]\_token & \\ \hline
4 & [napa auto repair]\_ORGANIZATION & \\ \hline
5 & [napa]\_ORGANIZATION  [auto repair]\_BUSINESS & \\ \hline
\end{tabular}
\end{center}
\label{table:diversity}
\end{table*}










%%89.31%


\section{Related Work}
NER has been a long time research area in natural language processing. Traditional NER target is text named entity. There are a lot of proposed methods in the fields, including all sort of machine learning methods. Traditionally, methods can be classified as unsupervised~\cite{Collins:1999,Etzioni:2005:UNE:1090483.1090487}, semi-supervised~\cite{Riloff:1999:LDI:315149.315364,Ji:2006:DSS:1641408.1641414} and supervised according to whether human labelled data is used in training. It can be support vector machine~\cite{Asahara:2003:JNE:1073445.1073447}, hidden markov model~\cite{Shen:NER}, maximum entropy~\cite{Chieu:2003:NER:1119176.1119199}, conditional random fields~\cite{McCallum:2003:ERN:1119176.1119206}  according to algorithm. The best NER result evaluated in the CoNLL  conferenece~\cite{TjongKimSang:2003:ICS:1119176.1119195} using NER shared-task data is 88.76%(F1) in terms of F1, where the authors used a multiple classifier combining method linear,HMM, transform based learning (TBL), and ME. 
Recently, deep learning method is found to achieve state-of-the-art results, 89.59%, on the same data~\cite{collobert:2011b} .

CRF is widely used in many NLP tasks including word segmentation~\cite{Xue:2003:CWS:1119250.1119278}, parsing~\cite{Sha:2003:SPC:1073445.1073473} and named entity recoginition~\cite{McCallum:2003:ERN:1119176.1119206}. In all these fields, CRF was found higher accuracy than the existing methods such as SVM, HMM and ME. Our work also proved this method was more effective. 

There are many existing work in text-related NER as mentioned above. Less literatures have been found for query NER. Actually, query NER is very important for search because 85\% of web search queries contains one or more entities~\cite{Lin:2012}. The work of \cite{Lin:2012,Pantel:2012:MET:2390524.2390603}  describes their approach to find query intent where EM algorithm is applied on large query set and using url-click information. In their work, 73 entity type is defined and 147K entities are extracted from Freesbase. Their work limits only one single entity inside a query. 


The paper \cite{Guo:2009:NER} proposes a probabilistic approach to the named entity recognition using query log data and Latent Dirichlet Allocation. Topic model is constructed by a weakly supervised LDA method. The method is tested on a tag set with only 4 taxonomies related to media. The best NER accuracy as reported is 80\% for the four categories. However, the evaluation was based on known query segmentation. 

There are some work on query segmentation which can be seen as entity extraction without finding entity type. 
\cite{Hagen:2012} described an overview of state-of-the-art work in query segmentation.  The best word segmentation results are around 85\%. 

Decision tree (DT) method was used in the early stage of NLP~\cite{Black:1993,Magerman95statisticaldecision-tree}. This approach was replaced by more advanced models such as maximum entropy and CRF. The old DT is a classification model that mixmizes classification error rate. The DT in this work is a regression model that optimizes editorial grades. Our DT model is based on the work of \cite{Friedman:2002,Ye:2009}, gradient boost decision tree (GBDT). The model has been widely applied into many machine learning tasks, for example, Web search ranking~\cite{Ye:2009}. 


\section{Overview of CRF tagging model}

We used three approach methods to generate query interpretation candidates: maximum length match (MLM), heuristic rule based (HRB), and CRF tagging. CRF tagging is the most effective method. But MLM and HRB are two necessary complementary methods. Our NER system has a knowledge base that saves all known entities. MLM is based on taxonomy match within a knowledge base. Candidate interpretations are generated if there are entities that match query words as long as possible. It is a greedy search. Top matches with higheset length of all possible matches are output. No.1, No.4 and No.5 of Table~\ref{table:diversity} can be generated by MLM because the query is either full match an entity or two entities. 

HRB is based on rules. We define hundreds of patterns. Each pattern defines a full parse consisting of words and entities. And a score is given to show likelihood of each pattern. For example, No.1 of Table~\ref{table:diversity} can be generated by "<s> PLACE + BUSINESS </s>" pattern. 



CRF approach has been studied for decades. The training and decoding methods used in this work have no much difference from the traditional methods, as pioneered by~\cite{lafferty:2001}. Therefore, we won't describe details about its theory. In short, 
CRF tagging model can be simply expressed as following formula,

\begin{equation}
p(y|x)=\frac{1}{Z(x)}\prod_{t=1}^{T}exp{\sum_{k=1}^{K}\theta_{k}f_{k}(y_{t},y_{t-1},x_{t})}
\label{eq:crf}
\end{equation}
For query interpretation task, $x$ and $y$ represent query sequence and taxonomy sequence respectively.

In the training stage, the model's parameter $\theta_{k}$ can be easily estimated by maximizing log likelihood of training data,

$$
\sum_{i=1}^{N}\sum_{t=1}^{T}\sum_{k=1}^{K}\theta_{k}f_{k}(y_{t}^{(i)},y_{t-1}^{(i)},x_{t}^{(i)})-\sum_{i=1}^{N}logZ(x^{(i)})
$$ 

CRF tagging includes entity boundary detection and taxonomy labeling. The two is completed in one step in this work. 
In the human labeled training data, entity boundary is tagged with ``B",``I" tags, indicating beginning/independent and intermediate position of an entity. Those tags are combined with a class name to indicate entity's span and category. For example, "B-organization I-organization" indicates a two term ORGANIZATION query. 

This work used many new features that is unique for query NER. Our model used about 30M features due to diversified web vocabulary and rich query context. We used the following information as features: precede/current/next query word, word position, word boundary, word spelling, lexical and query word topics. Topics were created by LDA (Latent Dirichlet Allocation) over 10 million queries. Our training data were selected randomly from query log and manually created by editors. 
In decoding stage, multiple interpretations can be produced and the order of interpretations can be ranked according to the value of $p(y|x)$. Usually, the top N-best results are used. 



We found using $p(y|x)$ as a criteria to select good interpretations was disappointed.  $p(y|x)$ is the ``qi\_prob" on Fig.\ref{Fig:NapaSnapshot} . An example is shown in Table~\ref{table:example}.
The table lists two example queries, ``napa" and ``dvd repair". The query ``napa" has two meanings, either organization name of automotive business or a city name of state California. While both of the top 2 interpretations in the table are correct, their $p(y|x)$ scores are very different. That makes hard to distinguish according to the value $p(y|x)$.
If we set threshold= $0.3$ so that both interpretation are chosen, the threshold is not appropriate for the second query, ``dvd repair" in which the first interpretation is a bad interpretation with a value above the threshold. Our CRF tagger mis-recognizes ``dvd repair" as a product at the first result, and assigned a probability of $0.45$. 
This interpretation will trigger Shopping DD if the first interpretation can't be rejected. Certainly, ``dvd repair" has no Shopping intent.



The above two examples show that we can't use $p(y|x)$ value to judge the interpretations.
We need another method to solve this problem. The new method can be a machine-learned classifier that have high performance to distinguish good and bad interpretations.
In this work, we used decision tree based classifier.


\section{Scorer: GBDT rescoring model}



The re-scoring function is a logistic regression model. The probability is given by the formula below,

\begin{equation}
p(x) = \frac{1}{1+e^{(a*(b-f(x))}} \Longrightarrow y=\left \{\begin{array}{cc} +1 & p(x) thrshld \\ -1 & othervise \end{array}\right.
\label{eq:gbdt}
\end{equation}

where $x$ is the generated feature vector for a given query and interpretation. $a$ and $b$ are the slope and pivot parameters. $a=2.0$ and $b=2.6$ are determined by optimizing classifier's performance in terms of recall and precision. $\{+1,-1\}$ denotes good interpretation and bad interpretation. $thrshld$ is set to separate good and bad interpretation according to the score $p(x)$. 

We employ Gradient Boosted Decision Tree algorithm 
to learn the function $f(x)$. Gradient Boosted Decision Tree is an
additive regression algorithm consisting of an ensemble of trees,
fitted to current residuals, gradients of the loss function, in a
forward step-wise manner. It iteratively fits an additive model as

%\begin{align}
$$
f_t(x) = T_t(x; \Theta) + \lambda \sum_{t=1}^T \beta_t T_t(x; \Theta_t)
$$
%\end{align}
\noindent such that certain loss function $L(y_i, f_T (x + i))$ is
minimized, where $T_t(x;\Theta_t)$ is a tree at iteration $t$,
weighted by parameter $\beta_t$, with a finite number of parameters,
$\Theta_t$ and $\lambda$ is the learning rate. At iteration t, tree
$T_t(x; \beta)$ is induced to fit the negative gradient by least
squares.


\noindent The optimal weights of trees $\beta_t$ are determined by
%\begin{align}
$$
\beta_t = argmin _{\beta} \sum_i^N L(y_i, f_{t-1}(x_i) + \beta T(x_i, \theta))
$$
%\end{align}


GBDT model is supervised machine learning. It needs manually created training data.
The process of creating training data is as follows. We randomly sampled hundreds of thousands of queries from Web search query log. For each query, we run CRF tagger to get top candidate interpretations. In addition, we generated more interpretations by maximum matching the entities defined in the lexicon. Editors assign 4 grades EGFB (Execllent, Good, Fair, Bad), to each of (query, interpretation) pairs. We show some examples on the last column of Table \ref{table:example}. 
Grade `E' is assigned to an excellent interpretation where all entities are correctly spanned and tagged. This interpretation will almost definitely lead to search results that are much better than those generated by the `default interpretation' (where all words are just `TOKEN').
Grade `G' is assigned to a good interpretation where some entities but not all are correctly spanned and tagged. For example, some entities will not be tagged at all or tagged with a very general tag. This interpretation may lead to search results that are better than the default interpretation, but these could be further improved.
%This is a broad category that can range from only one tag being correct and all other tags tokens, to all tags being correct but one of them failing to go to the second %hierarchy level.
Grade `F' is the baseline grade and default level of interpretation. It is no better or worse than if each word was treated as an individual keyword match. Queries with all spans marked as ``token" are considered Fair. The exception to this rule is a query whose only interpretation are all spans labeled ``token". In this case, the interpretation is marked Good.
Grade `B' is a bad interpretation; some of the important entities are not spanned or tagged correctly, in a way that leads to a wrong understanding of the query.




\section{Features of GBDT}

Features used in GBDT are completely different from those in CRF. GBDT feature value can't be string. Contextual features can't be used in GBDT. All GBDT features' values are real. Feature size is very different. We used 30M features in CRF, but only 500 in GBDT. But GBDT can use global features. This is a very good point for GBDT. We show some global features in Table.~\ref{table:features}. Those global features are not available in CRF. Some features are very useful. For example, No.1 is number of recognized entities. It is a good feature to  distinguish "napa" as PLACE from as ORGANIZATION. is No.3 is tag language model (LM) score for whole sentence. No.4 is count of generation methods that can generate the interpretation. Its function likes a voting strategy.   No.6 is number of entities stored in taxonomy.
\begin{table}
\begin{center}
\begin{tabular}{|c|l|}\hline
 1 & number of entities \\ \hline
 2 & number of non-recognized tokens \\ \hline
 3 & tag LM score of whole query \\ \hline
 4 & number of generation methods \\ \hline
 5 & crf model score \\ \hline
 6 & number of taxonomy match \\ \hline
\end{tabular}
\caption{GBDT global feature examples}
\end{center}
\end{table}

GBDT features are generated by the following methods.
Eight features are from query level. (1) number of terms (2) number of stop words (3) number of special characters (4) number of entities recognized for this interpretation (5) number of non-tagged terms (6) number of non-token tags (7) position of first non-token tag (8) position of last non-token tag.

Four features are from CRF tagger: (1) CRF raw probability (2) rank of CRF generated interpretation (3) ratio of current CRF interpretation to the top 1 CRF interpretation. (4) a boolean feature to indicate if interpretation is from CRF tagger.

There are two types of LM scores, context-independent tag (priors) and context dependent tag ngram LM scores. Context-independent tag scores are the average prior scores of any tags in the interpretation. Context-dependent tag sequence scores are ngram LM probability. Context-independent scores do not count token but context-dependent scores include tokens.
LM was built using editorial judgment data. 200 features are extracted by this method.

Some other features include template match. We manually wrote a lot of template-based rules to recognize query intents. These rules are from multiple domains. Take the query above as examples. (napa, PLACE) matches a Local template rule and (napa, ORGANIZATION) matches a Auto template rule. Features from template match include number of rule match.

Other features include counts of each taxonomy and ratio of each taxonomy if the query interpretation has multiple taxonomies. The feature size of this type is equal to the tag set. 

We defined entity confidence score based on CRF results. It is called global word posterior probability in ~\cite{Soong_generalizedword}. Assume CRF generates $N$ query interpretations. Entity $e$ may or may not have appearance in a interpretation $I_{i}$. If the CRF score of the interpretation is as $P(I_{i})$, then   the confidence of entity is calcuated by the formula as, 
$$
  conf(e) = \frac{\Sigma_{i}^{N} P(I_{i})\epsilon(e,I_{i})}{\Sigma_{i}^{N} P(I_i)} 
$$

Above is confidence score of one entity. From it, we genereate full interpretation score as a new feature. It is calculated as sum of all entities' confidence score recognized in the interpretation.

\section{Experiments}



\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
    \multicolumn{2}{|c|} {}    & \multicolumn{2}{|c|} { Coarse Category} & \multicolumn{2}{|c|}{ Fine Category} \\ \cline{3-6}
    \multicolumn{2}{|c|}{}      & Precision@N  &  Recall@N &   Precision@N & Recall@N \\ \hline
N=1 & CRF tagger &0.7249 & 0.6378 & 0.5987  &0.5268  \\ \cline{2-6}
         & Scorer  & 0.6814 & 0.633 & 0.5647 & 0.5246 \\ \hline
N=2 & CRF tagger & 0.4451 & 0.7587 & 0.3645&0.6213  \\ \cline{2-6}
         & Scorer  & 0.4606 &0.7209 & 0.3897 & 0.6099 \\ \hline
N=3 & CRF tagger &0.3094&0.8051 & 0.2541 &0.6613 \\ \cline{2-6}
         & Scorer  & 0.3186 & 0.7451 & 0.269 & 0.631 \\ \hline
\end{tabular}
\caption{Comparison of CRF tagger and Scorer regarding to perfect interpretation}
\label{table:pat1}
\end{center}
\end{table*}


We predefined an in-house taxonomy set. This set has two levels of granularity: coarse level and fine level. Coarse level has 20 entity categories and Fine level has about 200 categories. Most of the widely defined categories are included in our taxonomy set such as Local, Organization, Person, Product. There are also some new categories defined specifically for business purpose. Besides type Organization, we define Brand category to emphasize shopping intent. For instance, ``apple" is an Organization for query ``apple store", and BRAND for ``apple iphone". The differing categories makes clear to separate Local intent and Shopping intent. ``apple store" has Local intent that wants to show store location, driving direction, or store hours.  ``apple iphone" has Shopping intent that shows iphone type or price list. 

For each Coarse category, a few Fine categories are defined. As an example, we define Actor, Director, Politician for Person category. Similarly, each coase category is assoicated some fine category.

%As documented in the previous sections, our whole query understanding system has two main models, CRF tagger and Scorer. The two models use different training data. CRF %tagging model uses query perfect interpretation. GBDT model training uses 4-grade judgment. A few editors took two months to create 70k perfect interpretations and 200k %EGFB data. 



The evaluation set is 7K randomly sampled queries from a search query log. They are manually labeled perfect interpretation. 
Conventional NER evaluation method considers precision and recall regarding to best results. But for query due to ambigurity, we are more interested to multiple results.  
We define two metrics, $Precision@N$ and $Recall@N$, from top N results.  

$Precision@N$ is calculated as ``the number of queries that have extract match with answers on top N interpretations  divided by the number of  interpretations on top N". Each query has only one perfect interpretation. The perfect intepretation could be any of the top N. The higher N, the lower precision because all top N interpretations are counted in the denominator. 

 $Recall@N$ is the recall of perfect interpretation  on top N interpretations, calculated as ``number of queries that have extract match with answers on top N interpretations  divided by the number of queries".




Table~\ref{table:pat1} shows the comparison between CRF tagger and GBDT. Both coarse level and fine level results are presented. We list top 3 results.

 CRF tagger has higher precision@1 and recall@1. GBDT improves precision@2 and precision@3, but recall@2 and recall@3 are still won by the CRF tagger. However, our bucket test did show Scorer's results are better than CRF tagger's regarding to DD triggering.   
The second test may give a better explanation about this.   

The second test to compare CRF tagger and Scorer is to evaluate precision and recall of good interpretations. The test set is (query, interpretation) pairs evaluated with EGFB grades. The CRF tagger and Scorer assign scores to each (query, interpretation) pair. A threshold can be set to determine the boundary of good and bad interpretation according to the score. Precision is calculated by the number of correctly detected E and G interpretations divided by the total number of recognized E/G interpretations. Recall is calculated by the number of correctly detected E and G interpretations divided by the total number of E and G interpretations of answers. The precision recall graph is shown in Fig.~\ref{fig:precision}. Each point is the precision and recall values corresponding to a threshold. The top green curve is of GBDT and the bottom blue curve is CRF tagger. We see Scorer is much better than CRF tagger in terms of both precision and recall.


\begin{figure}
\centering
\epsfig{file=crf-gbdt-precision-recall.eps, height=3in, width=2in, angle=90}
\caption{Comparison of CRF and GBDT}
\label{fig:precision}
\end{figure}

As described in the previous sections, our NER system uses three methods to generate query interpretations. We found CRF approach gives the best results. Table~\ref{table:threemethods} shows individual method's results. The resutls are final results after  GBDT re-ranking.

 
\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
    \multicolumn{2}{|c|} {}    & \multicolumn{2}{|c|} { Coarse Category} & \multicolumn{2}{|c|}{ Fine Category} \\ \cline{3-6}
    \multicolumn{2}{|c|}{}      & Precision@N  &  Recall@N &   Precision@N & Recall@N \\ \hline
N=1 & MLM+GBDT &0.2986 & 0.2986 & 0.2916  &0.2916  \\ 
         & HRB+GBDT & 0.3914 & 0.3914 & 0.3513 & 0.3513 \\
        & CRF+GBDT  &0.5721 & 0.4689 & 0.5123  &0.4393  \\ \hline
N=2 & MLM+GBDT & 0.2302 & 0.3626 & 0.2121&0.3515  \\ 
         & HRB+GBDT  & 0.3218 &0.4325 & 0.2523 & 0.4112 \\ 
        & CRF+GBDT  & 0.4219 & 0.5325 & 0.3215&0.4976  \\  \hline
N=3 & MLM+GBDT &0.1620&0.3994 & 0.1450 &0.4013 \\ 
         & HRB+GBDT  & 0.2520 & 0.4816 & 0.1820 & 0.5212 \\
         & CRF+GBDT  &0.2897&0.5531 & 0.2432 &0.5331\\  \hline
\end{tabular}
\caption{Comparion of the three candidate generations}
\label{table:threemethods}
\end{center}
\end{table*}




So far, we observe a few issues: (1) the CRF tagger produces better perfect interpretation, seen in the firest test. (2) the Scorer gives better good interpretations, seen in the second test.  (3) The scores calculated by the Scorer can be better used by QP triggering than the CRF tagger, seen by bucket test and the second test because scores are used for thresholding.   (4) QP is effective for good interpretations too, not only for perfect interpretations. 

\section{Understanding QLAS results with the world's state-of-the-art}
We didn't find exactly matched references on query NER to compare with our work.Ttherefore we compared our system with two existing NER systems on different domain: Stanford NER~\footnote{http://nlp.stanford.edu/software/CRF-NER.shtml} and T-NER~\cite{Ritter:2011}. Table ~\ref{table:system comparison} lists results of all systems. Stanford NER was a supervised CRF tagger, trained using data from News domain. We applied it to our query domain evaluation data. Considering that queries in the data didn't have case information and also most of the queries were not proper sentences, it was not surprising that the results were much worse than ours. The second reference is T-NER, an NER for tweets. The query NER task is closer to tweet NER than news NER. The table lists the best results of T-NER from reference~\cite{Ritter:2011}. The comparison is based on three categories: PERSON, PLACE and ORGANIZATION. Our results are better than the two references. 

\begin{table*}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
& Stanford NER & T-NER & Ours\\ \hline
PLACE & 0.18 & 0.74 & 0.86 \\ \hline
PERSON & 0.28 & 0.82 & 0.82 \\ \hline
ORGANIZATION & 0.01 & 0.66 & 0.67\\\hline
\end{tabular}
\caption{F1 performances for the three known NER systems }
\label{table:system comparison}
\end{center}
\end{table*}

\section{Conclusions}
This paper tries to rationale of using Scorer in QLAS on top of CRF tagger while the CRF tagging method is widely used in many named entity recognition tasks with the highest accuracy. However, in Yahoo DD triggering, we found it was necessary to use Scorer, a gbdt-based reranking method. 
Not only we achieved positive bucket test with it, but the system has one more module to tune into higher performance. 
Our latest results on Scorer show significant improvement over the results reported in this paper. 
Actually, this work integrates two models, CRF tagger and GBDT models. The CRF tagger and GBDT function differently. CRF tagger can generate the most likely interpretations because it optimizes the likelihood of the training data.  The CRF tagger can integrate a great number of contextual features. GBDT is a regression model that maximizes EGFB grades.
It can make better judgment in classifying good and bad interpretations, but not good at generating good interpretations. 
By integrating the two models together, we derived the best results.  This idea was validated though online user experiences test. 

adobe flash player      dummy   autoscore: 4    adobe   brand_name(^taxonomy:brand_type=/manufacturer)  flash player    brand_name(^taxonomy:brand_type=/product_family)
adobe flash player      dummy   autoscore: 4    adobe   brand_name(^taxonomy:brand_type=/manufacturer)  flash player    product

ahmanson theatre        dummy   autoscore: 4    ahmanson theatre        organization_name
ahmanson theatre        dummy   autoscore: 4    ahmanson theatre        place_name(^taxonomy:place_category=/venue)

al ahram        dummy   autoscore: 4    al ahram        media_title(^taxonomy:media_category=/magazine_or_newspaper)
al ahram        dummy   autoscore: 4    al ahram        organization_name(^taxonomy:industry=/other)

albany rv       dummy   autoscore: 4    albany  place_name(^taxonomy:place_category=/city)      rv      product
albany rv       dummy   autoscore: 4    albany rv       organization_name(^taxonomy:industry=/automotive/automobile)


boomerang       dummy   autoscore: 4    boomerang       media_title(^taxonomy:media_category=/movie)
boomerang       dummy   autoscore: 4    boomerang       organization_name(^taxonomy:industry=/entertainment_arts/media)

broward county jail     dummy   autoscore: 4    broward county jail     organization_name(^taxonomy:industry=/government_politics)
broward county jail     dummy   autoscore: 4    broward county jail     place_name(^taxonomy:place_category=/poi)


buck rogers     dummy   autoscore: 4    buck rogers     media_title(^taxonomy:media_category=/movie)
buck rogers     dummy   autoscore: 4    buck rogers     media_title(^taxonomy:media_category=/tv_show)


cars    dummy   autoscore: 4    cars    media_title(^taxonomy:media_category=/movie)
cars    dummy   autoscore: 4    cars    product(^taxonomy:industry=/automotive/automobile)


zurich life insurance   dummy   autoscore: 4    zurich  organization_name(^taxonomy:industry=/other)    life insurance  business
zurich life insurance   dummy   autoscore: 4    zurich life insurance   organization_name(^taxonomy:industry=/finance)


windsor hotel   dummy   autoscore: 4    windsor place_name(^taxonomy:place_category=/city)      hotel   business(^taxonomy:industry=/travel/lodging)
windsor hotel   dummy   autoscore: 4    windsor hotel   organization_name(^taxonomy:industry=/travel/lodging)


discount tire   dummy   autoscore: 4    discount tire   organization_name(^taxonomy:industry=/other)
discount tire   dummy   autoscore: 4    discount tire   product(^taxonomy:industry=/automotive/automobile)

angus mcindoe   dummy   autoscore: 4    angus mcindoe   organization_name(^taxonomy:industry=/food_dining/restaurant)
angus mcindoe   dummy   autoscore: 4    angus mcindoe   person_name(^taxonomy:notability_category=/notnotable)


billy gilman    dummy   autoscore: 4    billy gilman    media_title(^taxonomy:media_category=/album)
billy gilman    dummy   autoscore: 4    billy gilman    person_name(^taxonomy:notability_category=/notable/arts_media/musician)

briar rabbit    dummy   autoscore: 4    briar rabbit    organization_name(^taxonomy:industry=/sports_recreation/other)
briar rabbit    dummy   autoscore: 4    briar rabbit    person_name(^taxonomy:notability_category=/notable/historic_or_fictional)

candlebox far behind    dummy   autoscore: 4    candlebox       organization_name(^taxonomy:industry=/entertainment_arts/music) far behind      media_title(^taxonomy:media_category=/song)
candlebox far behind    dummy   autoscore: 4    candlebox       person_name(^taxonomy:notability_category=/notable/arts_media/musician) far behind      media_title(^taxonomy:media_category=/song)

deep throat     dummy   autoscore: 4    deep throat     media_title(^taxonomy:media_category=/movie)
deep throat     dummy   autoscore: 4    deep throat     person_name(^taxonomy:notability_category=/notable/historic_or_fictional)

delta white     dummy   autoscore: 4    delta   brand_name(^taxonomy:brand_type=/manufacturer)  white   token
delta white     dummy   autoscore: 4    delta white     person_name(^taxonomy:notability_category=/notable/arts_media/actor)
\end{tabular}
\end{center}
\end{table*}


\bibliographystyle{acl}

\bibliography{ner}

\end{document}




